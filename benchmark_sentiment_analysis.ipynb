{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sk learn preprocessors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "# Sklearn models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, Activation, Conv1D, GlobalMaxPooling1D, BatchNormalization\n",
    "from keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Sklearn utility functions\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load base cnn\n",
    "from functions.models import get_base_cnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 CLASS CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data, remove the 'neutral' label and change -1 to 0 \n",
    "my_path = 'data/sentiment_data_1500_manual.csv'\n",
    "my_data = pd.read_csv(my_path, usecols = ['text', 'label'], encoding = 'latin-1')\n",
    "my_data = my_data.drop(my_data[my_data.label == 0].index).reset_index(drop = True)\n",
    "my_data.loc[my_data.label == -1, 'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global preprocessing\n",
    "\n",
    "# Replace upper letters with their lower letter counterparts\n",
    "my_data['text'] = my_data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove unnecessary stuff\n",
    "my_data['text'] = my_data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very easy pre processing according to sklearns CountVectorizer\n",
    "\n",
    "# Split the data into one list containing the tweets and one containing the labels\n",
    "my_tweets = my_data['text'].values.tolist()\n",
    "my_labels = my_data['label'].values.tolist()\n",
    "\n",
    "# Use a CountVectorizer to preprocess the data\n",
    "my_count_vec = CountVectorizer(analyzer = 'word')\n",
    "my_count_vec.fit(my_tweets)\n",
    "my_tweets = my_count_vec.transform(my_tweets)\n",
    "\n",
    "# Print this \n",
    "print('\\n')\n",
    "print('My tweets matrix:')\n",
    "print('\\n')\n",
    "print(my_tweets.toarray())\n",
    "print('\\n')\n",
    "print('min:', np.amin(my_tweets.toarray()), 'max:', np.amax(my_tweets.toarray()))\n",
    "print('\\n')\n",
    "print('mean:', np.mean(my_tweets.toarray()))\n",
    "print('\\n')\n",
    "print('unique elements:', len(np.unique(my_tweets.toarray())))\n",
    "print('\\n')\n",
    "print('dimensions:', my_tweets.toarray().shape)\n",
    "print('\\n')\n",
    "\n",
    "# Generate a train and val split \n",
    "my_train_prop = 0.66\n",
    "X_train, X_val, y_train, y_val = train_test_split(my_tweets, my_labels, \n",
    "                                                  train_size = my_train_prop, test_size  = 1 - my_train_prop, \n",
    "                                                  random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "my_bayes_mod = MultinomialNB(alpha = 1, fit_prior = True)\n",
    "my_bayes_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_bayes_mod_acc = accuracy_score(y_val, my_bayes_mod.predict(X_val))\n",
    "print('Naive Bayes accuracy:', my_bayes_mod_acc)\n",
    "\n",
    "# Create a logistic regression model\n",
    "my_reg_mod = LogisticRegression(penalty = 'l2', C = 1, solver = 'liblinear')\n",
    "my_reg_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_reg_mod_acc = accuracy_score(y_val, my_reg_mod.predict(X_val))\n",
    "print('Logistic regression accuracy:', my_reg_mod_acc)\n",
    "\n",
    "# Create classification tree\n",
    "my_tree_mod = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "my_tree_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_tree_mod_acc = accuracy_score(y_val, my_tree_mod.predict(X_val))\n",
    "print('Classification tree accuracy:', my_tree_mod_acc)\n",
    "\n",
    "# Create random forest\n",
    "my_forest_mod = RandomForestClassifier(criterion = 'gini', n_estimators = 500)\n",
    "my_forest_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_forest_mod_acc = accuracy_score(y_val, my_forest_mod.predict(X_val))\n",
    "print('Random forest accuracy:', my_forest_mod_acc)\n",
    "\n",
    "# Creating a gradient boosting model\n",
    "my_boosting_mod = GradientBoostingClassifier(loss = 'deviance', learning_rate = 0.01, n_estimators = 500)\n",
    "my_boosting_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_boosting_mod_acc = accuracy_score(y_val, my_boosting_mod.predict(X_val))\n",
    "print('Gradient boosting accuracy:', my_boosting_mod_acc)\n",
    "\n",
    "# Create support vector machine\n",
    "my_svm_mod = SVC(C = 100, kernel = 'rbf', gamma = 'auto')\n",
    "my_svm_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_svm_mod_acc = accuracy_score(y_val, my_svm_mod.predict(X_val))\n",
    "print('Support vector machine accuracy:', my_svm_mod_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Slightly more sophisticated pre processing according to sklearns CountVectorizer and td-idf \n",
    "\n",
    "# Split the data into one list containing the tweets and one containing the labels\n",
    "my_tweets = my_data['text'].values.tolist()\n",
    "my_labels = my_data['label'].values.tolist()\n",
    "\n",
    "# Use a CountVectorizer to preprocess the data\n",
    "my_count_vec = CountVectorizer(analyzer = 'word')\n",
    "my_count_vec.fit(my_tweets)\n",
    "my_tweets = my_count_vec.transform(my_tweets)\n",
    "\n",
    "# Use tf-idf for even smarter preprocessing\n",
    "my_tfidf = TfidfTransformer()\n",
    "my_tweets = my_tfidf.fit_transform(my_tweets)\n",
    "\n",
    "# Print this \n",
    "print('\\n')\n",
    "print('My tweets matrix:')\n",
    "print('\\n')\n",
    "print(my_tweets.toarray())\n",
    "print('\\n')\n",
    "print('min:', np.amin(my_tweets.toarray()), 'max:', np.amax(my_tweets.toarray()))\n",
    "print('\\n')\n",
    "print('mean:', np.mean(my_tweets.toarray()))\n",
    "print('\\n')\n",
    "print('unique elements:', len(np.unique(my_tweets.toarray())))\n",
    "print('\\n')\n",
    "print('dimensions:', my_tweets.toarray().shape)\n",
    "print('\\n')\n",
    "\n",
    "# Generate a train and val split \n",
    "my_train_prop = 0.66\n",
    "X_train, X_val, y_train, y_val = train_test_split(my_tweets, my_labels, \n",
    "                                                  train_size = my_train_prop, test_size  = 1 - my_train_prop, \n",
    "                                                  random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "my_bayes_mod = MultinomialNB(alpha = 1, fit_prior = True)\n",
    "my_bayes_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_bayes_mod_acc = accuracy_score(y_val, my_bayes_mod.predict(X_val))\n",
    "print('Naive Bayes accuracy:', my_bayes_mod_acc)\n",
    "\n",
    "# Create a logistic regression model\n",
    "my_reg_mod = LogisticRegression(penalty = 'l2', C = 1, solver = 'liblinear')\n",
    "my_reg_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_reg_mod_acc = accuracy_score(y_val, my_reg_mod.predict(X_val))\n",
    "print('Logistic regression accuracy:', my_reg_mod_acc)\n",
    "\n",
    "# Create classification tree\n",
    "my_tree_mod = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "my_tree_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_tree_mod_acc = accuracy_score(y_val, my_tree_mod.predict(X_val))\n",
    "print('Classification tree accuracy:', my_tree_mod_acc)\n",
    "\n",
    "# Create random forest\n",
    "my_forest_mod = RandomForestClassifier(criterion = 'gini', n_estimators = 500)\n",
    "my_forest_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_forest_mod_acc = accuracy_score(y_val, my_forest_mod.predict(X_val))\n",
    "print('Random forest accuracy:', my_forest_mod_acc)\n",
    "\n",
    "# Creating a gradient boosting model\n",
    "my_boosting_mod = GradientBoostingClassifier(loss = 'deviance', learning_rate = 0.01, n_estimators = 500)\n",
    "my_boosting_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_boosting_mod_acc = accuracy_score(y_val, my_boosting_mod.predict(X_val))\n",
    "print('Gradient boosting accuracy:', my_boosting_mod_acc)\n",
    "\n",
    "# Create support vector machine\n",
    "my_svm_mod = SVC(C = 100, kernel = 'rbf', gamma = 'auto')\n",
    "my_svm_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_svm_mod_acc = accuracy_score(y_val, my_svm_mod.predict(X_val))\n",
    "print('Support vector machine accuracy:', my_svm_mod_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very sophisticated Keras Tokenizer\n",
    "\n",
    "# Two hyperparameters for the tokenizer\n",
    "tokenizer_max_features = 5000\n",
    "tokenizer_maxlen = 40\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = tokenizer_max_features, split = ' ')\n",
    "tokenizer.fit_on_texts(my_data['text'].values)\n",
    "\n",
    "# Use the tokenizer\n",
    "my_tweets = tokenizer.texts_to_sequences(my_data['text'].values)\n",
    "my_tweets = pad_sequences(my_tweets, maxlen = tokenizer_maxlen)\n",
    "my_labels = my_data['label']\n",
    "\n",
    "# Generate a train and val split \n",
    "my_train_prop = 0.66\n",
    "X_train, X_val, y_train, y_val = train_test_split(my_tweets, my_labels,\n",
    "                                                  train_size = my_train_prop, test_size  = 1 - my_train_prop, \n",
    "                                                  random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "my_bayes_mod = MultinomialNB(alpha = 1, fit_prior = True)\n",
    "my_bayes_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_bayes_mod_acc = accuracy_score(y_val, my_bayes_mod.predict(X_val))\n",
    "my_bayes_mod_conf = confusion_matrix(y_val, my_bayes_mod.predict(X_val))\n",
    "print('Naive Bayes accuracy:', my_bayes_mod_acc)\n",
    "print('Naive Bayes confmatrix:')\n",
    "print(my_bayes_mod_conf)\n",
    "\n",
    "# Create a logistic regression model\n",
    "my_reg_mod = LogisticRegression(penalty = 'l2', C = 1, solver = 'liblinear')\n",
    "my_reg_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_reg_mod_acc = accuracy_score(y_val, my_reg_mod.predict(X_val))\n",
    "my_reg_mod_conf = confusion_matrix(y_val, my_reg_mod.predict(X_val))\n",
    "print('Logistic regression accuracy:', my_reg_mod_acc)\n",
    "print('LogReg confmatrix:')\n",
    "print(my_reg_mod_conf)\n",
    "\n",
    "# Create classification tree\n",
    "my_tree_mod = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "my_tree_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_tree_mod_acc = accuracy_score(y_val, my_tree_mod.predict(X_val))\n",
    "my_tree_mod_conf = confusion_matrix(y_val, my_tree_mod.predict(X_val))\n",
    "print('Classification tree accuracy:', my_tree_mod_acc)\n",
    "print('Classification Tree confmatrix:')\n",
    "print(my_tree_mod_conf)\n",
    "\n",
    "\n",
    "# Create random forest\n",
    "my_forest_mod = RandomForestClassifier(criterion = 'gini', n_estimators = 500, verbose = 2)\n",
    "my_forest_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_forest_mod_acc = accuracy_score(y_val, my_forest_mod.predict(X_val))\n",
    "my_forest_mod_conf = confusion_matrix(y_val, my_forest_mod.predict(X_val))\n",
    "print('Random forest accuracy:', my_forest_mod_acc)\n",
    "print('Random forest confmatrix:')\n",
    "print(my_forest_mod_conf)\n",
    "\n",
    "# Creating a gradient boosting model\n",
    "my_boosting_mod = GradientBoostingClassifier(loss = 'deviance', learning_rate = 0.01, n_estimators = 500, verbose = 2)\n",
    "my_boosting_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_boosting_mod_acc = accuracy_score(y_val, my_boosting_mod.predict(X_val))\n",
    "my_boosting_mod_conf = confusion_matrix(y_val, my_boosting_mod.predict(X_val))\n",
    "print('Gradient boosting accuracy:', my_boosting_mod_acc)\n",
    "print('Gradient boosting confmatrix:')\n",
    "print(my_boosting_mod_conf)\n",
    "\n",
    "# Create support vector machine\n",
    "my_svm_mod = SVC(C = 100, kernel = 'rbf', gamma = 'auto', verbose = 2)\n",
    "my_svm_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_svm_mod_acc = accuracy_score(y_val, my_svm_mod.predict(X_val))\n",
    "my_svm_mod_conf = confusion_matrix(y_val, my_svm_mod.predict(X_val))\n",
    "print('Support vector machine accuracy:', my_svm_mod_acc)\n",
    "print('Support vector machine confmatrix:')\n",
    "print(my_svm_mod_conf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## CNN\n",
    "print('\\n')\n",
    "print('Training CNN..')\n",
    "print('\\n')\n",
    "\n",
    "# Model hyperparameters\n",
    "my_max_features = tokenizer_max_features\n",
    "my_embedding_dims = 50\n",
    "my_maxlen = tokenizer_maxlen\n",
    "\n",
    "my_filters = 16\n",
    "my_kernel_size = 3\n",
    "num_hidden_dims = 16\n",
    "\n",
    "# Get model Architecture\n",
    "my_cnn = get_base_cnn(max_features = my_max_features, \n",
    "                      embedding_dims = my_embedding_dims, \n",
    "                      maxlen = my_maxlen, \n",
    "                      num_conv_filters = my_filters, \n",
    "                      kernel_size = my_kernel_size, \n",
    "                      num_hidden_dims = num_hidden_dims)\n",
    "\n",
    "# Compile the model\n",
    "my_cnn.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['categorical_accuracy'])\n",
    "\n",
    "# training hyperparameters\n",
    "num_batch_size = 8\n",
    "num_epochs = 10\n",
    "\n",
    "# Fit the model\n",
    "my_model = my_cnn.fit(X_train, y_train,\n",
    "                      batch_size = num_batch_size,\n",
    "                      epochs = num_epochs,\n",
    "                      validation_data = (X_val, y_val),\n",
    "                      verbose = 1)\n",
    "\n",
    "print('CNN accuracy:', my_model.history['val_acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 CLASS CASE (BASE MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data, remove the 'neutral' label and change -1 to 0 because only idiots use negative integers for labels\n",
    "my_path = 'data/sentiment_data_900_manual.csv'\n",
    "my_data = pd.read_csv(my_path, usecols = ['text', 'label'], encoding = 'latin-1')\n",
    "my_data.loc[my_data.label == 1, 'label'] = 2\n",
    "my_data.loc[my_data.label == 0, 'label'] = 1\n",
    "my_data.loc[my_data.label == -1, 'label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Global preprocessing\n",
    "\n",
    "# Replace upper letters with their lower letter counterparts\n",
    "my_data['text'] = my_data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove unnecessary stuff\n",
    "my_data['text'] = my_data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very sophisticated Keras Tokenizer\n",
    "\n",
    "# Two hyperparameters for the tokenizer\n",
    "tokenizer_max_features = 5000\n",
    "tokenizer_maxlen = 40\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = tokenizer_max_features, split = ' ')\n",
    "tokenizer.fit_on_texts(my_data['text'].values)\n",
    "\n",
    "# Use the tokenizer\n",
    "my_tweets = tokenizer.texts_to_sequences(my_data['text'].values)\n",
    "my_tweets = pad_sequences(my_tweets, maxlen = tokenizer_maxlen)\n",
    "my_labels = my_data['label']\n",
    "\n",
    "# Generate a train and val split \n",
    "my_train_prop = 0.66\n",
    "X_train, X_val, y_train, y_val = train_test_split(my_tweets, my_labels,\n",
    "                                                  train_size = my_train_prop, test_size  = 1 - my_train_prop, \n",
    "                                                  random_state = 1338)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Naive Bayes model\n",
    "my_bayes_mod = MultinomialNB(alpha = 1, fit_prior = True)\n",
    "my_bayes_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_bayes_mod_acc = accuracy_score(y_val, my_bayes_mod.predict(X_val))\n",
    "print('Naive Bayes accuracy:', my_bayes_mod_acc)\n",
    "\n",
    "# Create a logistic regression model\n",
    "my_reg_mod = LogisticRegression(penalty = 'l2', C = 1, solver = 'newton-cg', multi_class = 'multinomial')\n",
    "my_reg_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_reg_mod_acc = accuracy_score(y_val, my_reg_mod.predict(X_val))\n",
    "print('Logistic regression accuracy:', my_reg_mod_acc)\n",
    "\n",
    "# Create classification tree\n",
    "my_tree_mod = tree.DecisionTreeClassifier(criterion = 'gini')\n",
    "my_tree_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_tree_mod_acc = accuracy_score(y_val, my_tree_mod.predict(X_val))\n",
    "print('Classification tree accuracy:', my_tree_mod_acc)\n",
    "\n",
    "# Create random forest\n",
    "my_forest_mod = RandomForestClassifier(criterion = 'gini', n_estimators = 500)\n",
    "my_forest_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_forest_mod_acc = accuracy_score(y_val, my_forest_mod.predict(X_val))\n",
    "print('Random forest accuracy:', my_forest_mod_acc)\n",
    "\n",
    "# Creating a gradient boosting model\n",
    "my_boosting_mod = GradientBoostingClassifier(loss = 'deviance', n_estimators = 500,\n",
    "                                             learning_rate = 0.01, subsample = 0.75, max_features = 30)\n",
    "my_boosting_mod.fit(X_train, y_train)\n",
    "# Predict validation data and compute accuracy\n",
    "my_boosting_mod_acc = accuracy_score(y_val, my_boosting_mod.predict(X_val))\n",
    "print('Gradient boosting accuracy:', my_boosting_mod_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 CLASS CASE (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data for 3 classes (0,1,2)\n",
    "my_path = 'data/sentiment_data_1500_manual.csv'\n",
    "my_data = pd.read_csv(my_path, usecols = ['text', 'label'], encoding = 'latin-1')\n",
    "my_data.loc[my_data.label == 1, 'label'] = 2\n",
    "my_data.loc[my_data.label == 0, 'label'] = 1\n",
    "my_data.loc[my_data.label == -1, 'label'] = 0\n",
    "\n",
    "my_labels = my_data['label'].values\n",
    "my_labels = to_categorical(my_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Very sophisticated Keras Tokenizer\n",
    "\n",
    "# Two hyperparameters for the tokenizer\n",
    "tokenizer_max_features = 5000\n",
    "tokenizer_maxlen = 40\n",
    "\n",
    "# Create the tokenizer\n",
    "tokenizer = Tokenizer(num_words = tokenizer_max_features, split = ' ')\n",
    "tokenizer.fit_on_texts(my_data['text'].values)\n",
    "\n",
    "# Use the tokenizer\n",
    "my_tweets = tokenizer.texts_to_sequences(my_data['text'].values)\n",
    "my_tweets = pad_sequences(my_tweets, maxlen = tokenizer_maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a train and val split \n",
    "my_train_prop = 0.66\n",
    "X_train, X_val, y_train, y_val = train_test_split(my_tweets, my_labels, \n",
    "                                                  train_size = my_train_prop, test_size  = 1 - my_train_prop, \n",
    "                                                  random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN\n",
    "print('\\n')\n",
    "print('Training CNN..')\n",
    "print('\\n')\n",
    "\n",
    "# Model hyperparameters\n",
    "my_max_features = tokenizer_max_features\n",
    "my_embedding_dims = 5\n",
    "my_maxlen = tokenizer_maxlen\n",
    "\n",
    "my_filters = 64\n",
    "my_kernel_size = 3\n",
    "num_hidden_dims = 32\n",
    "\n",
    "# Get model Architecture\n",
    "my_cnn = get_base_cnn(max_features = my_max_features, \n",
    "                      embedding_dims = my_embedding_dims, \n",
    "                      maxlen = my_maxlen, \n",
    "                      num_conv_filters = my_filters, \n",
    "                      kernel_size = my_kernel_size, \n",
    "                      num_hidden_dims = num_hidden_dims)\n",
    "\n",
    "# Compile the model\n",
    "my_cnn.compile(loss = 'categorical_crossentropy',\n",
    "               optimizer = 'adam',\n",
    "               metrics = ['categorical_accuracy'])\n",
    "\n",
    "# training hyperparameters\n",
    "num_batch_size = 8\n",
    "num_epochs = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "my_model = my_cnn.fit(X_train, y_train,\n",
    "                      batch_size = num_batch_size,\n",
    "                      epochs = num_epochs,\n",
    "                      validation_data = (X_val, y_val),\n",
    "                      verbose = 1)\n",
    "\n",
    "print('CNN accuracy:', my_model.history['val_categorical_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "from models import confusion_matrix # not the same as confusion_matrix from sklearn, so we import it here\n",
    "\n",
    "#Calculate prediction of CNN (provides \"probability\" of each label)\n",
    "prediction = my_cnn.predict(X_val)\n",
    "\n",
    "# Choose \"safest\" label\n",
    "my_pred = np.argmax(prediction, axis=1)\n",
    "my_ground_truth = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Conf Matrix\n",
    "conf = confusion_matrix(my_pred, my_ground_truth, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy score\n",
    "np.sum(np.diag(conf))/np.sum(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Sentiment Env)",
   "language": "python",
   "name": "sentiment_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
