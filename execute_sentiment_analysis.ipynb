{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from functions.load_data import load_tweets, load_labelled_data\n",
    "from functions.tokenizer import apply_keras_tokenizer, load_model_and_tokenizer\n",
    "from functions.data_modification import aggregate_sentiment\n",
    "from functions.apply_models import apply_model\n",
    "from functions.preprocessing import preprocess_tweets, concat_tweet_files\n",
    "from functions.sentiwordnet import SentimentAnalysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all tweets into different variables\n",
    "coinlist = ['ADA', 'BCH','BCN','DASH','EOS','ETC','ETH','ICX','IOT','LTC','NEO','QTUM','TRX','VEN','XEM','XLM','XMR','XRP','ZEC']\n",
    "\n",
    "# Load the model and tokenizer which were used to train the model\n",
    "my_model, my_tokenizer = load_model_and_tokenizer('models_and_tokenizers/randomforest.sav', 'models_and_tokenizers/tokenizer_1500_man.sav')\n",
    "\n",
    "# Load SentiWordNet\n",
    "s = SentimentAnalysis(filename='data/SentiWordNet.txt',weighting='geometric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply both Method 1 and 2 to the tweets and aggregate them\n",
    "for f in coinlist:\n",
    "    filename = f + '_semantic_filtered.csv'\n",
    "    my_tweets = pd.read_csv('data/filtered_tweets/' + filename)\n",
    "    \n",
    "    # Make the tweets and the header all lowercase\n",
    "    my_tweets = preprocess_tweets(my_tweets)\n",
    "    #my_tweets.columns = my_tweets.columns.str.lower()\n",
    "    \n",
    "    # Tokenize the tweets using the tokenizer\n",
    "    my_tokenized_tweets = apply_keras_tokenizer(my_tweets, my_tokenizer)\n",
    "    \n",
    "    # Label the tweets by using the model\n",
    "    my_tweets = apply_model(my_tweets, my_tokenized_tweets, my_model)\n",
    "\n",
    "    # Apply the sentiwordnet evaluation\n",
    "    my_tweets['sentiwordnet'] = [s.score(tweet) for tweet in my_tweets['text']]\n",
    "    \n",
    "    # Save the .csv\n",
    "    pd.DataFrame.to_csv(my_tweets, 'data/labelled_tweets/' + f + '_randomforest_sentiwordnet_labelled.csv')\n",
    "    \n",
    "\n",
    "    \n",
    "     # Aggregate the tweets hourly\n",
    "    aggregated_tweets = aggregate_sentiment(my_tweets, '1H')\n",
    "    \n",
    "    # Save the aggregated tweets to a .csv file\n",
    "    pd.DataFrame.to_csv(aggregated_tweets, 'data/aggregated_tweets/' + f + '_sentiment_aggr_1h_shifted.csv')\n",
    "    \n",
    "    # Aggregate the tweets 6h\n",
    "    aggregated_tweets = aggregate_sentiment(my_tweets, '6H')\n",
    "    \n",
    "    # Save the aggregated tweets to a .csv file\n",
    "    pd.DataFrame.to_csv(aggregated_tweets, 'data/aggregated_tweets/' + f + '_sentiment_aggr_6h_shifted.csv')\n",
    "    \n",
    "     # Aggregate the tweets 12h\n",
    "    aggregated_tweets = aggregate_sentiment(my_tweets, '12H')\n",
    "    \n",
    "    # Save the aggregated tweets to a .csv file\n",
    "    pd.DataFrame.to_csv(aggregated_tweets, 'data/aggregated_tweets/' + f + '_sentiment_aggr_12h_shifted.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Sentiment Env)",
   "language": "python",
   "name": "sentiment_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
